from keras.layers import Conv2D, Input, Reshape, RepeatVector, concatenate, UpSampling2D, Flatten, Conv2DTranspose
from keras.models import Model

from keras import backend as K
from keras.losses import mean_squared_error
from keras.optimizers import Adam






import keras
import keras.backend as K

from keras.models import Sequential
from keras.layers import Convolution2D, ZeroPadding2D
from keras.layers.normalization import BatchNormalization
from keras.layers import Activation, Flatten, Dense, Reshape

from keras.activations import softmax
from keras.optimizers import Adam

import functools
softmax3 = functools.partial(softmax, axis = 3)
softmax3.__name__ = 'softmax3'

AB_BIN_WEIGHTS = [0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022683875960096394, 0.0022701514488164282, 0.0022701514488164282, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022639899323660677, 0.002260484059354882, 0.002266626482016857, 0.002267506697060341, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002267506697060341, 0.002241394204601768, 0.0022336773300481384, 0.002264868100464008, 0.0022319696796154315, 0.0022091693402192733, 0.00215137779747977, 0.0021529643109304136, 0.002226862358492229, 0.0022578617718606478, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022613595092805684, 0.0022217783577366725, 0.0021876494360075008, 0.0022008425383961473, 0.002119361883231732, 0.001908124001954105, 0.0015624533084842031, 0.0015641274780188452, 0.0018263578991269066, 0.0021124611282973037, 0.0022422549277471057, 0.002267506697060341, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0021721866155591757, 0.0018738325673884526, 0.0015738240091854817, 0.0016545424775093061, 0.0016419801578567428, 0.001260559793541178, 0.0007593906077023949, 0.00060332375291495, 0.000819156768135748, 0.0014648509985228488, 0.002009239879317096, 0.0022200888455365334, 0.0022613595092805684, 0.0022683875960096394, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0020581061158842207, 0.0013661451187699577, 0.0007553628351225719, 0.0005590741993200676, 0.0004547491868847851, 0.000398049236181538, 0.00023146745471483223, 0.00021732977932181793, 0.0003341530002419387, 0.0007380806653742855, 0.001466322440609064, 0.001975255039905242, 0.0021466322553506103, 0.0022311168331513747, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0021577379219253423, 0.0010867164211620312, 0.00030931777182422273, 0.00013504651065775655, 7.518975659822808e-05, 3.878197354580426e-05, 2.755266024810407e-05, 8.05747510329907e-05, 0.00017891292915754315, 0.000450782049522368, 0.001029794009741643, 0.0016953766052291062, 0.0020823270502972244, 0.0022016723928287964, 0.002250031286774244, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0021958765192853637, 0.0012835592576171864, 0.00048152542855748855, 0.00027423419491804524, 0.0002145507775384476, 0.000136431746035496, 4.981242983661773e-05, 5.209962021900112e-05, 8.733972423518476e-05, 0.00019081080267606437, 0.0005185129464406754, 0.0011487157597858174, 0.0017513114594162626, 0.002149002406578356, 0.0022302646381919342, 0.0022578617718606478, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002264868100464008, 0.0018877662878872517, 0.0011207148792924501, 0.0009062458863609847, 0.0010600816144021954, 0.001260831989027692, 0.0013444407164114804, 0.0010686180136602764, 0.0006063309483055392, 0.00038485271132143874, 0.00039282727913843367, 0.0007050179758789724, 0.00126740012988185, 0.0018183951608801811, 0.002094276923041832, 0.0021934019043175863, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002103329723802333, 0.0018483268084508954, 0.0016948844902988963, 0.001746596708149173, 0.002010623613959107, 0.002174613565008838, 0.0021239874885492873, 0.002059558000272694, 0.0017682831626510353, 0.0012771022739579163, 0.0009620922113074756, 0.0010709700352637198, 0.0015126663545154825, 0.0019462854307141576, 0.002174613565008838, 0.0022474331808693838, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002245704441065276, 0.0021078855367371156, 0.0020705127749715627, 0.002115522574493357, 0.002217559389596289, 0.0022526354066213913, 0.0022561169575854565, 0.002241394204601768, 0.0022234704433702164, 0.0021754237537596802, 0.0020508772912494057, 0.0017976812110477038, 0.0015763733432476706, 0.0016355411690438131, 0.001975923466232843, 0.0021967026322445384, 0.002260484059354882, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022294130939908502, 0.0022158762853535306, 0.0022396747391991503, 0.0022631124449967854, 0.0022683875960096394, 0.0022639899323660677, 0.0022561169575854565, 0.0022631124449967854, 0.0022543748379181446, 0.002244841068097983, 0.002177045943708313, 0.002164938161538321, 0.002102572335566923, 0.0021147563813558173, 0.002093526039260358, 0.0022091693402192733, 0.0022613595092805684, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.002264868100464008, 0.0022596092870003468, 0.002266626482016857, 0.0022710344042723954, 0.0022692691796621298, 0.002266626482016857, 0.002264868100464008, 0.002262235637564948, 0.002266626482016857, 0.0022657469500830473, 0.0022543748379181446, 0.0022158762853535306, 0.0022074989430052538, 0.0022192450526400736, 0.002235387595482995, 0.002258735191430638, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0022692691796621298, 0.0022701514488164282, 0.0022710344042723954, 0.0022692691796621298, 0.0022683875960096394, 0.002267506697060341, 0.0022639899323660677, 0.002245704441065276, 0.0022474331808693838, 0.0022422549277471057, 0.0022657469500830473, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0022657469500830473, 0.0022692691796621298, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022657469500830473, 0.0022613595092805684, 0.0022657469500830473, 0.002262235637564948, 0.0022422549277471057, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022692691796621298, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022701514488164282, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954, 0.0022710344042723954]
RZHANG_LOSS_WEIGHT = 1000




mse_weight = 1.0 #1e-3

# set these to zeros to prevent learning
perceptual_weight = 1. / (2. * 128. * 128.) # scaling factor
attention_weight = 1.0 # 1.0



# shows the minimum value of the AB channels
def y_true_min(yt, yp):
    return K.min(yt)

 # shows the maximum value of the RGB AB channels
def y_true_max(yt, yp):
    return K.max(yt)



# shows the minimum value of the predicted AB channels
def y_pred_min(yt, yp):
    return K.min(yp)


# shows the maximum value of the predicted AB channels
def y_pred_max(yt, yp):
    return K.max(yp)


def gram_matrix(x):
    assert K.ndim(x) == 4

    with K.name_scope('gram_matrix'):
        if K.image_data_format() == "channels_first":
            batch, channels, width, height = K.int_shape(x)
            features = K.batch_flatten(x)
        else:
            batch, width, height, channels = K.int_shape(x)
            features = K.batch_flatten(K.permute_dimensions(x, (0, 3, 1, 2)))

        gram = K.dot(features, K.transpose(features)) # / (channels * width * height)
    return gram


def l2_norm(x):
    return K.sqrt(K.sum(K.square(x)))


def attention_vector(x):
    if K.image_data_format() == "channels_first":
        batch, channels, width, height = K.int_shape(x)
        filters = K.batch_flatten(K.permute_dimensions(x, (1, 0, 2, 3)))  # (channels, batch*width*height)
    else:
        batch, width, height, channels = K.int_shape(x)
        filters = K.batch_flatten(K.permute_dimensions(x, (3, 0, 1, 2)))  # (channels, batch*width*height)

    filters = K.mean(K.square(filters), axis=0)  # (batch*width*height,)
    filters = filters / l2_norm(filters)  # (batch*width*height,)
    return filters


def total_loss(y_true, y_pred):
    mse_loss = mse_weight * mean_squared_error(y_true, y_pred)
    perceptual_loss = perceptual_weight * K.sum(K.square(gram_matrix(y_true) - gram_matrix(y_pred)))
    attention_loss = attention_weight * l2_norm(attention_vector(y_true) - attention_vector(y_pred))

    return mse_loss + perceptual_loss + attention_loss


def custom_loss_rebalancing_segments(y_true, y_pred):
    y_pred_clipped = K.clip(y_pred, K.epsilon(), None)

    img_height = y_true.shape[1]
    img_width = y_true.shape[2]

    loss = -K.sum(y_true * K.log(y_pred_clipped), axis = 3)

    bin_indices = K.argmax(y_true, axis = 3)
    weights = K.gather(reference = AB_BIN_WEIGHTS, indices = bin_indices)

    loss = RZHANG_LOSS_WEIGHT * K.sum(loss * weights)

    loss += K.sum((y_pred[:, :-1, :, :] - y_pred[:, 1:, :, :]) ** 2)
    loss += K.sum((y_pred[:, :, :-1, :] - y_pred[:, :, 1:, :]) ** 2)

    return loss


def custom_loss(y_true, y_pred):
    y_pred_clipped = K.clip(y_pred, K.epsilon(), None)
    return -K.sum(y_true * K.log(y_pred_clipped))


def custom_loss_rebalancing(y_true, y_pred):
    y_pred_clipped = K.clip(y_pred, K.epsilon(), None)

    num_images = y_true.shape[0]
    img_height = y_true.shape[1]
    img_width = y_true.shape[2]

    loss = -K.sum(y_true * K.log(y_pred_clipped), axis = 3)

    bin_indices = K.argmax(y_true, axis = 3)
    weights = K.gather(reference = AB_BIN_WEIGHTS, indices = bin_indices)

    loss = RZHANG_LOSS_WEIGHT * K.sum(loss * weights)

    return loss

def generar_modelo(lr=1e-3, img_size=256):

    # encoder model
    encoder_ip = Input(shape=(img_size, img_size, 1))
    encoder1 = Conv2D(64, (3, 3), padding='same', activation='relu', strides=(2, 2))(encoder_ip)
    encoder1 = Conv2D(64, (3, 3), padding='same', activation='relu', strides=(2, 2))(encoder1)
    encoder = Conv2D(128, (3, 3), padding='same', activation='relu')(encoder1)
    encoder2 = Conv2D(128, (3, 3), padding='same', activation='relu', strides=(2, 2))(encoder)
    encoder = Conv2D(256, (3, 3), padding='same', activation='relu')(encoder2)
    encoder = Conv2D(256, (3, 3), padding='same', activation='relu', strides=(2, 2))(encoder)
    encoder = Conv2D(512, (3, 3), padding='same', activation='relu')(encoder)
    encoder = Conv2D(256, (3, 3), padding='same', activation='relu')(encoder)

    # decoder model
    decoder = Conv2D(128, (3, 3), padding='same', activation='relu')(encoder)
    decoder = UpSampling2D((2,2))(decoder)
    decoder = Conv2D(64, (3, 3), padding='same', activation='relu')(decoder)
    decoder = UpSampling2D((2,2))(decoder)
    decoder = Conv2D(32, (3, 3), padding='same', activation='relu')(decoder)
    decoder = Conv2DTranspose(2, (3, 3), strides=(2, 2), padding='same', activation='tanh')(decoder)
    decoder = UpSampling2D((2,2))(decoder)
    model = Model([encoder_ip], decoder, name='Colorizer')
    model.compile(optimizer=Adam(lr), loss='mse', metrics=[y_true_max,
                                                                y_true_min,
                                                                y_pred_max,
                                                                y_pred_min])

    print("Colorization model built and compiled")
    return model


if __name__ == '__main__':
    model = generar_modelo()
    model.summary()

    from keras.utils.vis_utils import plot_model

    plot_model(model, to_file='skip_model.png', show_shapes=True)
